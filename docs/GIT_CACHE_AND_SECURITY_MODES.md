Here’s a focused description of the **Git cache idea** and the distinction between **online** and **gatekept** projects, plus just the **relevant bits of code** from `src/codexctl/lib` that implement this logic.

Update (host-initialized cache and container wiring)
- New command: `codexctl cache-init <project>` initializes or updates a host-side git mirror of the upstream using the project’s SSH config generated by `codexctl ssh-init`.
- Online mode: containers clone from the local cache first (fast, no secrets), then the working copy’s `origin` is re-pointed to the real upstream for future fetch/push.
- Gatekept mode: containers use the local cache as their only visible remote (`CODE_REPO=file:///.../cache.git`); upstream credentials/URLs are not mounted into the container.
- Optional: you can completely shield containers in online mode from SSH by setting `ssh.mount_in_online: false` in project.yml; in that case the container can still clone from a local cache configured to require credentials.

---

## Concept: Git cache + two security modes

### 1. Three layers: upstream → cache → tasks

For each project you conceptually have three Git layers:

1. **Upstream (real remote)**

   * The canonical repo: GitHub, GitLab, internal Git server, etc.
   * Example: `https://gitlab.com/electric-sheep/ultimate-container.git`.

2. **Local cache (host-side)**

   * A clone or mirror of the upstream, stored under the project’s state directory, e.g.:

     * `STATE_ROOT/projects/<project_id>/cache/upstream.git`
   * Purpose:

     * Speed up **clone** operations (no repeated large network transfers).
     * In gatekept setups, act as the **only thing** the `codexctl` tooling fetches from/pushes to before a human intervenes.

3. **Task working copy (container)**

   * A regular Git working directory underneath each task’s workspace, bind-mounted into the container as `/workspace`.
   * Each **task** gets its own **isolated repo** (its own `.git`), seeded from either:

     * the real upstream (online mode), or
     * the local cache (gatekept mode).

---

### 2. Online vs Gatekept projects

The project’s **security mode** controls how tasks interact with upstream vs cache:

#### Online projects

* **Goal:** the agent behaves like a normal “developer”:

  * It can push branches directly to upstream (subject to repo permissions).
  * It can use `gh` or other GitHub/GitLab tooling normally.
* Typical behaviour:

  * `CODE_REPO` inside the container points to the **upstream URL**.
  * If a local cache exists, the container will seed the initial clone from it (`CLONE_FROM=file:///.../cache.git`) and then repoint `origin` to upstream.
  * The cache is a performance accelerator in online mode; security comes from normal upstream auth.
* In the current code:

  * For online projects, we set `CODE_REPO` to `upstream_url` and optionally mount SSH credentials for direct pushes.

#### Gatekept projects

* **Goal:** agent’s changes must **not** reach the canonical repo directly.

  * Tasks only interact with a host-side cache mirror (no direct upstream access inside the container).
  * Humans (or other reviewing agents) can promote changes from the cache to upstream.
* Typical behaviour:

  * Host maintains a mirror clone under the project’s cache path.
  * Container sees only this cache mirror as `CODE_REPO`:

    * `CODE_REPO=file:///git-cache/cache.git`
  * Container **never sees** upstream URLs, nor any upstream credentials.
* In the current code:

  * For gatekept projects, the container bind-mounts the host cache mirror (read-write) and uses it as the sole remote.

* **Optional SSH mount** (`ssh.mount_in_gatekeeping`):

  * By default, containers in gatekeeping mode have no SSH access.
  * Set `ssh.mount_in_gatekeeping: true` in project.yml to mount SSH credentials while still enforcing the cache-only model for the main repository.
  * This is useful for repos with git submodules that need to be fetched from private repositories.
  * The user should ensure the SSH key does not have write access to upstream repositories.

### Host-side cache lifecycle
1. Generate SSH material for private upstreams (optional for public HTTPS):
   - `codexctl ssh-init <project>`
2. Initialize or update the cache mirror:
   - `codexctl cache-init <project>` (use `--force` to recreate)
3. Run tasks:
   - Online: container clones from cache then talks to upstream directly.
   - Gatekept: container talks only to the cache mirror.

---

## Relevant code snippets (from `src/codexctl/lib`)

Below are only the pieces related to:

* `security_class` (`online` vs `gatekept`),
* cache/staging roots,
* how `CODE_REPO` is set for CLI/UI tasks.

### 1. Project model: upstream, security mode, cache, staging

```python
from dataclasses import dataclass
from pathlib import Path
from typing import Optional

from .paths import config_root, state_root  # FHS-aware roots
import yaml

@dataclass
class Project:
    id: str
    security_class: str          # "online" | "gatekept"
    upstream_url: Optional[str]
    default_branch: str
    root: Path

    tasks_root: Path             # workspace dirs
    cache_path: Path             # future: local cache / mirror
    staging_root: Optional[Path] # gatekept only

    ssh_key_name: Optional[str]
    ssh_host_dir: Optional[Path]
    codex_config_dir: Optional[Path]


def _find_project_root(project_id: str) -> Path:
    user_root = Path("...")  # user project root, omitted here
    sys_root = config_root() / "projects" / project_id
    # (user root handling omitted for brevity)
    if (sys_root / "project.yml").is_file():
        return sys_root
    raise SystemExit(f"Project '{project_id}' not found")


def load_project(project_id: str) -> Project:
    root = _find_project_root(project_id)
    cfg_path = root / "project.yml"
    cfg = yaml.safe_load(cfg_path.read_text()) or {}

    proj_cfg = cfg.get("project", {}) or {}
    git_cfg = cfg.get("git", {}) or {}
    ssh_cfg = cfg.get("ssh", {}) or {}
    codex_cfg = cfg.get("codex", {}) or {}
    tasks_cfg = cfg.get("tasks", {}) or {}
    cache_cfg = cfg.get("cache", {}) or {}
    gate_cfg = cfg.get("gatekeeping", {}) or {}

    pid = proj_cfg.get("id", project_id)
    sec = proj_cfg.get("security_class", "online")

    sr = state_root()
    tasks_root = Path(tasks_cfg.get("root", sr / "tasks" / pid)).resolve()
    cache_path = Path(cache_cfg.get("path", sr / "cache" / f"{pid}.git")).resolve()

    staging_root: Optional[Path] = None
    if sec == "gatekept":
        staging_root = Path(gate_cfg.get("staging_root", sr / "stage" / pid)).resolve()

    upstream = git_cfg.get("upstream_url")
    default_branch = git_cfg.get("default_branch", "main")

    ssh_key_name = ssh_cfg.get("key_name")
    ssh_host_dir = ssh_cfg.get("host_key_dir")
    ssh_host_dir_path = Path(ssh_host_dir).resolve() if ssh_host_dir else None

    codex_dir = codex_cfg.get("host_config_dir")
    codex_dir_path = Path(codex_dir).resolve() if codex_dir else None

    return Project(
        id=pid,
        security_class=sec,
        upstream_url=upstream,
        default_branch=default_branch,
        root=root,
        tasks_root=tasks_root,
        cache_path=cache_path,
        staging_root=staging_root,
        ssh_key_name=ssh_key_name,
        ssh_host_dir=ssh_host_dir_path,
        codex_config_dir=codex_dir_path,
    )
```

> Note: `cache_path` is where you would place a `upstream.git` mirror; in the current prototype it isn’t used yet for clone acceleration, but the directory is defined here for that purpose.

---

### 2. Task creation: gatekept staging dir vs online

When a new task is created, we treat **online** and **gatekept** differently for staging:

```python
def task_new(project_id: str) -> str:
    import datetime, random, string

    project = load_project(project_id)
    project.tasks_root.mkdir(parents=True, exist_ok=True)

    ts = datetime.datetime.now().strftime("%Y%m%dT%H%M%S")
    suffix = "".join(random.choices(string.ascii_lowercase + string.digits, k=4))
    task_id = f"{ts}-{suffix}"

    workspace = project.tasks_root / task_id
    workspace.mkdir(parents=True, exist_ok=True)

    meta = {
        "task_id": task_id,
        "project_id": project.id,
        "security_class": project.security_class,
        "workspace": str(workspace),
        "status": "created",
        # "mode" will be set on first run (cli/ui)
    }

    if project.security_class == "gatekept":
        # Create a host-side staging repo dir
        if not project.staging_root:
            raise SystemExit("gatekept project missing staging_root")
        stage_tasks = project.staging_root / "tasks"
        stage_tasks.mkdir(parents=True, exist_ok=True)
        staging_repo = stage_tasks / f"{task_id}.git"
        staging_repo.mkdir(parents=True, exist_ok=True)
        meta["staging_repo"] = str(staging_repo)

    _write_task_meta(project, task_id, meta)
    return task_id
```

* For **online projects**: no staging repo is created; the task’s repo will point to the upstream directly.
* For **gatekept projects**: a dedicated `staging_repo` path is created for each task.

---

### 3. Building `CODE_REPO` and volumes for CLI/UI runs

The real logic that *enforces* online vs gatekept is here:

```python
def _build_task_env_and_volumes(project: Project, task_id: str, meta: dict):
    env = [
        "REPO_ROOT=/workspace",
        f"GIT_BRANCH={project.default_branch}",
    ]
    vols = []

    workspace = Path(meta.get("workspace", project.tasks_root / task_id))
    if not workspace.is_dir():
        raise SystemExit(f"Workspace does not exist: {workspace}")
    vols += ["-v", f"{workspace}:/workspace:Z"]

    # Codex config
    if project.codex_config_dir and project.codex_config_dir.is_dir():
        vols += ["-v", f"{project.codex_config_dir}:/home/dev/.codex:Z"]

    # SSH (only online projects get upstream keys)
    if project.security_class == "online":
        if project.ssh_host_dir and project.ssh_host_dir.is_dir():
            vols += ["-v", f"{project.ssh_host_dir}:/home/dev/.ssh:Z"]
        if project.ssh_key_name:
            env.append(f"SSH_KEY_NAME={project.ssh_key_name}")

    # CODE_REPO: differs by security_class
    if project.security_class == "online":
        if not project.upstream_url:
            raise SystemExit("online project requires git.upstream_url")
        env.append(f"CODE_REPO={project.upstream_url}")
    else:  # gatekept
        staging_repo = meta.get("staging_repo")
        if not staging_repo:
            raise SystemExit("gatekept task missing staging_repo in meta")
        vols += ["-v", f"{staging_repo}:/git-remote/{task_id}.git:Z"]
        env.append(f"CODE_REPO=file:///git-remote/{task_id}.git")

    return env, vols
```

* **Online projects**:

  * `CODE_REPO` → **real upstream** URL.
  * If SSH keys are configured, they are mounted into the container and `SSH_KEY_NAME` is set.
  * The container’s startup script (`init-ssh-and-repo.sh`) will do a normal `git clone $CODE_REPO $REPO_ROOT`.
* **Gatekept projects**:

  * The *host-side* `staging_repo` is bind-mounted to `/git-remote/<task-id>.git`.
  * `CODE_REPO` is set to `file:///git-remote/<task-id>.git`.
  * No upstream URL nor keys are given; the container can **only** talk to that staging path.

That’s the core of the gatekeeping: remote vs file URL.

---

### 4. Running tasks (CLI vs UI) using that env/volume config

For completeness, this is how the CLI and UI runs use the above env/vols:

```python
def task_run_cli(project_id: str, task_id: str) -> None:
    project = load_project(project_id)
    meta = _load_task_meta(project, task_id)
    _check_mode(meta, "cli")  # ensure we don't mix ui/cli on same task

    pod_name = ensure_project_pod(project)  # one pod per project
    env, vols = _build_task_env_and_volumes(project, task_id, meta)

    meta["mode"] = "cli"
    meta["status"] = "running-cli"
    _write_task_meta(project, task_id, meta)

    container_name = f"{project.id}-cli-{task_id}"
    image = f"{project.id}-codex:latest"

    cmd = ["podman", "run", "--rm", "-it", "--pod", pod_name, "--name", container_name]
    for e in env:
        cmd += ["-e", e]
    cmd += vols
    cmd.append(image)

    _run_cmd(cmd)

    meta["status"] = "finished-cli"
    _write_task_meta(project, task_id, meta)


def task_run_ui(project_id: str, task_id: str) -> None:
    project = load_project(project_id)
    meta = _load_task_meta(project, task_id)
    _check_mode(meta, "ui")

    env, vols = _build_task_env_and_volumes(project, task_id, meta)
    port = _assign_ui_port(project, task_id, meta)

    meta["mode"] = "ui"
    meta["status"] = "running-ui"
    _write_task_meta(project, task_id, meta)

    container_name = f"{project.id}-ui-{task_id}"
    image = f"{project.id}-codexui:latest"

    cmd = [
        "podman", "run", "--rm",
        "--name", container_name,
        "-p", f"127.0.0.1:{port}:7860",
    ]
    for e in env:
        cmd += ["-e", e]
    cmd += vols
    cmd.append(image)

    print(f"UI for task {task_id} → http://127.0.0.1:{port}/")
    _run_cmd(cmd)

    meta["status"] = "finished-ui"
    _write_task_meta(project, task_id, meta)
```

Again, **all** of the security / caching semantics boil down to how `_build_task_env_and_volumes` sets up:

* env `CODE_REPO` → upstream vs staging,
* volumes:

  * only workspace & codex config,
  * plus SSH keys for **online**,
  * plus **staging repo** for **gatekept**.

The **local cache** lives in `project.cache_path` and can be used in future iterations as a `--reference` or `--mirror` optimization, without changing this high-level model.

---

### Mental model

* **Online**:

  * containers trust the real upstream,
  * cache is a performance optimization.
* **Gatekept**:

  * containers trust only host-local staging repos,
  * real upstream is “air-gapped” behind human review,
  * local cache is both performance and guardrail (mirror of upstream that you control on the host).

That’s the entire story in a nutshell.
